name: Validate Agent Reports

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'run_agent.py'
      - 'agent_report_schema.json'
      - 'schema_validator.py'
      - 'reports/**/*.json'
      - 'packages/agents/**/*.py'
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  check-ollama-models:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama service
        run: |
          ollama serve &
          for i in {1..10}; do
            ollama list && break
            sleep 1
          done

      - name: Check Ollama models
        id: check-models
        run: |
          python scripts/wait_for_ollama.py --models phi3.5:latest --timeout 60

  validate-reports:
    runs-on: ubuntu-latest
    needs: check-ollama-models
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install jsonschema
          pip install -r requirements.txt

      - name: Validate agent report schema
        id: validate-schema
        run: |
          python validate_schema_ci.py
          if [ $? -eq 0 ]; then echo "✅ Passed" > schema_validation_status.txt; else echo "❌ Failed" > schema_validation_status.txt; fi

      - name: Test agent pipeline validation
        id: test-pipeline
        run: |
          python test_agent_pipeline.py
          if [ $? -eq 0 ]; then echo "✅ Passed" > pipeline_test_status.txt; else echo "❌ Failed" > pipeline_test_status.txt; fi

      - name: Run schema validation tests
        id: run-tests
        run: |
          python -m pytest test_schema_validator.py -v
          if [ $? -eq 0 ]; then echo "✅ Passed" > schema_tests_status.txt; else echo "❌ Failed" > schema_tests_status.txt; fi

      - name: Generate validation summary
        if: always()
        run: |
          echo "# Agent Report Validation Results" > validation_report.md
          echo "" >> validation_report.md

          # Check schema validation status
          if [ -f "schema_validation_status.txt" ]; then
            STATUS=$(cat schema_validation_status.txt)
            echo "- Schema validation: $STATUS" >> validation_report.md
          else
            echo "- Schema validation: Status unknown" >> validation_report.md
          fi

          # Check pipeline test status
          if [ -f "pipeline_test_status.txt" ]; then
            STATUS=$(cat pipeline_test_status.txt)
            echo "- Pipeline tests: $STATUS" >> validation_report.md
          else
            echo "- Pipeline tests: Status unknown" >> validation_report.md
          fi

          # Check schema tests status
          if [ -f "schema_tests_status.txt" ]; then
            STATUS=$(cat schema_tests_status.txt)
            echo "- Schema tests: $STATUS" >> validation_report.md
          else
            echo "- Schema tests: Status unknown" >> validation_report.md
          fi

      - name: Upload validation report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-report
          path: validation_report.md

  test-agents:
    runs-on: ubuntu-latest
    needs: check-ollama-models
    strategy:
      matrix:
        agent: [TestGeneratorAgent, DependencyAgent, CodeAnalyzerAgent, CodeDebuggerAgent, CodeRepairAgent, IntegratedCodebaseOptimizer]
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create test file
        run: |
          mkdir -p test_samples
          echo 'def test_function():
              print("Hello World")
              return True' > test_samples/sample_code.py

      - name: Run ${{ matrix.agent }}
        run: |
          # Create output directory
          mkdir -p test_outputs/${{ matrix.agent }}

          # Determine correct input type
          INPUT_FILE="test_samples/sample_code.py"
          if [[ "${{ matrix.agent }}" == "DependencyAgent" || "${{ matrix.agent }}" == "CodeAnalyzerAgent" || "${{ matrix.agent }}" == "IntegratedCodebaseOptimizer" ]]; then
            INPUT_PATH="test_samples"
          else
            INPUT_PATH="$INPUT_FILE"
          fi

          # Run the agent
          python run_agent.py ${{ matrix.agent }} $INPUT_PATH --output-dir=test_outputs/${{ matrix.agent }}

      - name: Generate code coverage report
        run: |
          pip install pytest pytest-cov

          # Run pytest with appropriate test files
          if [[ -f "tests/test_${{ matrix.agent }}.py" ]]; then
            pytest --cov=packages.agents --cov-report=xml tests/test_${{ matrix.agent }}.py -v
          elif [[ -f "tests/test_$(echo ${{ matrix.agent }} | tr '[:upper:]' '[:lower:]').py" ]]; then
            pytest --cov=packages.agents --cov-report=xml tests/test_$(echo ${{ matrix.agent }} | tr '[:upper:]' '[:lower:]').py -v
          else
            # Run general tests if specific tests don't exist
            pytest --cov=packages.agents --cov-report=xml tests/test_agents.py -v
          fi

      - name: Upload coverage report
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          name: ${{ matrix.agent }}-coverage
          fail_ci_if_error: false
